{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "659ee46f",
   "metadata": {},
   "source": [
    "# Add-one Smoothing (Laplace’s law)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f3aeffbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the required module\n",
    "import nltk\n",
    "from nltk import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "37452a92",
   "metadata": {},
   "outputs": [],
   "source": [
    "datas = \"start I am Sam end\",\"start Sam I am end\",\"start I do not like green eggs end\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "86b6b406",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenization\n",
    "sent_tokens = []\n",
    "for data in datas:\n",
    "    sent_token = sent_tokenize(data)\n",
    "    sent_tokens.append(sent_token)\n",
    "word_tokens = []\n",
    "for sent_token in sent_tokens :\n",
    "    word_token = []\n",
    "    for sentence in sent_token:\n",
    "        sentence = sentence.translate(str.maketrans('', '', string.punctuation))\n",
    "        word_token += word_tokenize(sentence)\n",
    "    word_tokens.append(word_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a4922c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stopwords removal    \n",
    "stops = set(stopwords.words('english'))\n",
    "word_token = []\n",
    "\n",
    "for w in word_tokens:\n",
    "    words = []\n",
    "    for word in w:\n",
    "        if word.lower() not in stops:\n",
    "            words.append(word)\n",
    "    word_token.append(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "087619ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['start', 'I', 'am', 'Sam', 'end'],\n",
       " ['start', 'Sam', 'I', 'am', 'end'],\n",
       " ['start', 'I', 'do', 'not', 'like', 'green', 'eggs', 'end']]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "781cc3a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'start': 3, 'I': 3, 'am': 2, 'Sam': 2, 'end': 3, 'do': 1, 'not': 1, 'like': 1, 'green': 1, 'eggs': 1}\n",
      "size of Vacablory:  10\n"
     ]
    }
   ],
   "source": [
    "# caculating few required numeric datas frequency and prob\n",
    "c = 0\n",
    "unique_words = []\n",
    "frequency = {}\n",
    "for words in word_tokens:\n",
    "    for word in words:\n",
    "        c+=1\n",
    "        if word not in frequency:\n",
    "            unique_words.append(word)\n",
    "            frequency[word]=1\n",
    "        else:\n",
    "            frequency[word]+=1\n",
    "\n",
    "print(frequency)\n",
    "V = len(frequency)\n",
    "print(\"size of Vacablory: \",V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f5f95aed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{('start', 'I'): 2, ('I', 'am'): 2, ('am', 'Sam'): 1, ('Sam', 'end'): 1, ('start', 'Sam'): 1, ('Sam', 'I'): 1, ('am', 'end'): 1, ('I', 'do'): 1, ('do', 'not'): 1, ('not', 'like'): 1, ('like', 'green'): 1, ('green', 'eggs'): 1, ('eggs', 'end'): 1}\n",
      "TOTAL UNIQUE BIGRAMS : 13\n"
     ]
    }
   ],
   "source": [
    "#generating bigrams\n",
    "bigramss = []\n",
    "for words in word_tokens:\n",
    "    bigrams = zip(words[:-1],words[1:])\n",
    "    bigramss.append(bigrams)\n",
    "\n",
    "# Caluculating frequency and prob of bigrams\n",
    "bigram_freq = {}\n",
    "bigram_count = 0\n",
    "for bigrams in bigramss :\n",
    "    for bigram in bigrams:\n",
    "        bigram_count += 1\n",
    "        if bigram in bigram_freq :\n",
    "            bigram_freq[bigram] += 1\n",
    "        else :\n",
    "            bigram_freq[bigram] = 1\n",
    "bigram_prop = {}\n",
    "for bigram, freq in bigram_freq.items() : \n",
    "    bigram_prop[bigram] = freq/bigram_count\n",
    "print(bigram_freq)\n",
    "print(\"TOTAL UNIQUE BIGRAMS :\", len(bigram_freq))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cecb1304",
   "metadata": {},
   "source": [
    "# Laplace’s law"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f2057eca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P_add1:\n",
      " {('start', 'I'): 0.23076923076923078, ('I', 'am'): 0.23076923076923078, ('am', 'Sam'): 0.16666666666666666, ('Sam', 'end'): 0.16666666666666666, ('start', 'Sam'): 0.15384615384615385, ('Sam', 'I'): 0.16666666666666666, ('am', 'end'): 0.16666666666666666, ('I', 'do'): 0.15384615384615385, ('do', 'not'): 0.18181818181818182, ('not', 'like'): 0.18181818181818182, ('like', 'green'): 0.18181818181818182, ('green', 'eggs'): 0.18181818181818182, ('eggs', 'end'): 0.18181818181818182}\n"
     ]
    }
   ],
   "source": [
    "# add1 smoothing for all events\n",
    "P_add1 = {}\n",
    "for bigram, freq in bigram_freq.items() : \n",
    "    P_add1[bigram] =  (freq+1)/(frequency[bigram[0]]+V)\n",
    "print(\"P_add1:\\n\",P_add1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "05f55769",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probability of query: (P_add1):  6.724045185583647e-05\n"
     ]
    }
   ],
   "source": [
    "# Query : SASTRA UNIVERSITY is a known institution\n",
    "query = \"start I am Sam green end\"\n",
    "q_word_tokens =word_tokenize(query)\n",
    "q_bigrams = zip(q_word_tokens[:-1],q_word_tokens[1:])\n",
    "p_add1_q = 1\n",
    "for q_bigram in q_bigrams:\n",
    "    if q_bigram in P_add1:\n",
    "        p_add1_q *= P_add1[q_bigram]\n",
    "    elif q_bigram[0] in frequency:\n",
    "        p_add1_q *= (1/(frequency[q_bigram[0]]+V))\n",
    "print(\"Probability of query: (P_add1): \", p_add1_q)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
